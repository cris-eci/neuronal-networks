{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture Assignment - CIFAR-10\n",
    "**Student Assignment**: Neural Networks Course\n",
    "\n",
    "This notebook explores convolutional neural networks using the CIFAR-10 dataset.\n",
    "\n",
    "**Dataset**: CIFAR-10 (Canadian Institute For Advanced Research)\n",
    "- 60,000 32x32 color images in 10 classes\n",
    "- 50,000 training images and 10,000 test images\n",
    "- Classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Dataset Exploration (EDA)\n",
    "Loading and analyzing the CIFAR-10 dataset to understand its structure and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset Size and Dimensions\n",
    "**Analysis:**\n",
    "- Total samples: 60,000 (50,000 train + 10,000 test)\n",
    "- Image dimensions: 32x32 pixels with 3 color channels (RGB)\n",
    "- This is significantly more complex than MNIST (28x28 grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar([class_names[i] for i in unique], counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "for i, count in zip(unique, counts):\n",
    "    print(f\"{class_names[i[0]]}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Class Distribution\n",
    "**Analysis:**\n",
    "- The dataset is perfectly balanced with 5,000 samples per class\n",
    "- No class imbalance issues to address\n",
    "- This simplifies training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    # Find first instance of each class\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(x_train[idx])\n",
    "    plt.title(class_names[i])\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample Images from Each Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visual Inspection\n",
    "**Analysis:**\n",
    "- Images show significant variability in pose, lighting, and background\n",
    "- Low resolution (32x32) makes some images challenging even for humans\n",
    "- Color information is important for distinguishing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pixel value distribution\n",
    "print(f\"Pixel value range: [{x_train.min()}, {x_train.max()}]\")\n",
    "print(f\"Mean pixel value: {x_train.mean():.2f}\")\n",
    "print(f\"Std pixel value: {x_train.std():.2f}\")\n",
    "\n",
    "# Plot pixel distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(x_train.flatten(), bins=50, alpha=0.7)\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Pixel Value Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preprocessing Requirements\n",
    "**Analysis:**\n",
    "- Pixel values range from 0 to 255 (standard RGB format)\n",
    "- **Normalization needed**: Divide by 255 to scale to [0, 1] range\n",
    "- This helps neural networks train faster and more stably\n",
    "- No resizing needed as all images are already 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization\n",
    "x_train_normalized = x_train.astype('float32') / 255.0\n",
    "x_test_normalized = x_test.astype('float32') / 255.0\n",
    "\n",
    "print(f\"Normalized range: [{x_train_normalized.min()}, {x_train_normalized.max()}]\")\n",
    "print(f\"Shape preserved: {x_train_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Why CIFAR-10 is Appropriate for Convolutional Layers\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "1. **Spatial Structure**: Images have strong spatial relationships between neighboring pixels. Convolutions can exploit local patterns like edges, textures, and shapes.\n",
    "\n",
    "2. **Translation Invariance**: Objects can appear at different positions in the image. Convolutional layers with weight sharing provide translation invariance.\n",
    "\n",
    "3. **Hierarchical Features**: \n",
    "   - Early layers can detect edges and simple patterns\n",
    "   - Middle layers can detect parts (wings, wheels)\n",
    "   - Deeper layers can recognize complete objects\n",
    "\n",
    "4. **Parameter Efficiency**: A fully connected layer for 32×32×3 images would have massive parameter count. Convolutions reduce this dramatically while maintaining expressive power.\n",
    "\n",
    "5. **Color Channels**: RGB channels provide additional information that convolutions can process efficiently across all channels simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Baseline Model (Non-Convolutional)\n",
    "*To be implemented: Flatten + Dense layers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Convolutional Architecture Design\n",
    "*To be implemented: Custom CNN architecture*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Controlled Experiments\n",
    "*To be implemented: Systematic exploration of one architectural aspect*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Interpretation and Reasoning\n",
    "*To be completed: Analysis of results and architectural decisions*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
